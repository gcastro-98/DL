{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "UR10PKuEN_FY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "155f8bae-0fb2-41de-cde1-a1ac5759794a"
      },
      "source": [
        "'''\n",
        "This example demonstrates the use of Convolution1D for text classification.\n",
        "'''\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.layers import Convolution1D, MaxPooling1D\n",
        "from tensorflow.keras.datasets import imdb\n",
        "\n",
        "\n",
        "# set parameters:\n",
        "max_features = 5000\n",
        "maxlen = 100\n",
        "batch_size = 32\n",
        "embedding_dims = 100\n",
        "nb_filter = 250\n",
        "filter_length = 5\n",
        "hidden_dims = 250\n",
        "nb_epoch = 10\n",
        "\n",
        "print('Loading data...')\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "print(len(X_train), ' train sequences \\n')\n",
        "print(len(X_test), ' test sequences \\n')\n",
        "\n",
        "print('Pad sequences (samples x time)')\n",
        "X_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
        "X_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n",
        "print('X_train shape:', X_train.shape)\n",
        "print('X_test shape:', X_test.shape)\n",
        "\n",
        "print('Build model...')\n",
        "model = Sequential()\n",
        "\n",
        "# we start off with an efficient embedding layer which maps\n",
        "# our vocab indices into embedding_dims dimensions\n",
        "model.add(Embedding(max_features, embedding_dims, input_length=maxlen))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "# we add a Convolution1D, which will learn filters\n",
        "# word group filters of size filter_length:\n",
        "model.add(Convolution1D(filters=nb_filter,\n",
        "                        kernel_size=filter_length,\n",
        "                        padding='valid',\n",
        "                        activation='relu'))\n",
        "# we use standard max pooling (halving the output of the previous layer):\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "model.add(Convolution1D(filters=nb_filter,\n",
        "                        kernel_size=filter_length,\n",
        "                        padding='valid',\n",
        "                        activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "\n",
        "# We flatten the output of the conv layer,\n",
        "# so that we can add a vanilla dense layer:\n",
        "model.add(Flatten())\n",
        "\n",
        "# We add a vanilla hidden layer:\n",
        "model.add(Dense(hidden_dims))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
        "model.add(Dense(1))\n",
        "model.add(Activation('sigmoid'))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['accuracy'])\n",
        "model.fit(X_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=nb_epoch,\n",
        "          validation_data=(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n",
            "17473536/17464789 [==============================] - 0s 0us/step\n",
            "25000  train sequences \n",
            "\n",
            "25000  test sequences \n",
            "\n",
            "Pad sequences (samples x time)\n",
            "X_train shape: (25000, 100)\n",
            "X_test shape: (25000, 100)\n",
            "Build model...\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 100, 100)          500000    \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 100, 100)          0         \n",
            "_________________________________________________________________\n",
            "conv1d (Conv1D)              (None, 96, 250)           125250    \n",
            "_________________________________________________________________\n",
            "max_pooling1d (MaxPooling1D) (None, 48, 250)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 44, 250)           312750    \n",
            "_________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1 (None, 22, 250)           0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 5500)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 250)               1375250   \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 250)               0         \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 250)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 251       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 1)                 0         \n",
            "=================================================================\n",
            "Total params: 2,313,501\n",
            "Trainable params: 2,313,501\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "782/782 [==============================] - 45s 17ms/step - loss: 0.4352 - accuracy: 0.7830 - val_loss: 0.3263 - val_accuracy: 0.8600\n",
            "Epoch 2/10\n",
            "782/782 [==============================] - 13s 16ms/step - loss: 0.3023 - accuracy: 0.8766 - val_loss: 0.3081 - val_accuracy: 0.8682\n",
            "Epoch 3/10\n",
            "782/782 [==============================] - 15s 19ms/step - loss: 0.2615 - accuracy: 0.8947 - val_loss: 0.4135 - val_accuracy: 0.8378\n",
            "Epoch 4/10\n",
            "782/782 [==============================] - 12s 16ms/step - loss: 0.2319 - accuracy: 0.9118 - val_loss: 0.3519 - val_accuracy: 0.8603\n",
            "Epoch 5/10\n",
            "782/782 [==============================] - 15s 19ms/step - loss: 0.2023 - accuracy: 0.9240 - val_loss: 0.3207 - val_accuracy: 0.8666\n",
            "Epoch 6/10\n",
            "782/782 [==============================] - 12s 16ms/step - loss: 0.1747 - accuracy: 0.9358 - val_loss: 0.4883 - val_accuracy: 0.8500\n",
            "Epoch 7/10\n",
            "782/782 [==============================] - 12s 16ms/step - loss: 0.1471 - accuracy: 0.9476 - val_loss: 0.3769 - val_accuracy: 0.8606\n",
            "Epoch 8/10\n",
            "782/782 [==============================] - 12s 16ms/step - loss: 0.1262 - accuracy: 0.9549 - val_loss: 0.5192 - val_accuracy: 0.8293\n",
            "Epoch 9/10\n",
            "782/782 [==============================] - 15s 19ms/step - loss: 0.1030 - accuracy: 0.9643 - val_loss: 0.4594 - val_accuracy: 0.8568\n",
            "Epoch 10/10\n",
            "782/782 [==============================] - 15s 19ms/step - loss: 0.0836 - accuracy: 0.9704 - val_loss: 0.4579 - val_accuracy: 0.8371\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7faeaaed9b90>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    }
  ]
}